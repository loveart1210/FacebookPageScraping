from selenium import webdriver
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
import re

from urllib.parse import urlparse, parse_qs, urlencode

import random
import json
import time

PAGE_URL = "https://www.facebook.com/profile.php?id=61555234277669"
OUTPUT_FILE = "hnmu_posts_2.json"
COOKIES_FILE = "cookies.json"

# S·ªë b√†i mu·ªën crawl
crawl_post = 5

# Map ƒë√∫ng expiry t·ª´ EditThisCookie (c√≥ tr∆∞·ªùng 'expirationDate')
def load_cookies(driver, cookies_file):
    with open(cookies_file, "r", encoding="utf-8") as f:
        cookies = json.load(f)
        for cookie in cookies:
            c = {
                "name": cookie["name"],
                "value": cookie["value"],
                "domain": cookie.get("domain", ".facebook.com"),
                "path": cookie.get("path", "/"),
                "secure": cookie.get("secure", True),
                "httpOnly": cookie.get("httpOnly", False),
            }
            # EditThisCookie d√πng 'expirationDate' (float gi√¢y). Selenium ch·∫•p nh·∫≠n 'expiry' (int).
            if "expirationDate" in cookie:
                try:
                    c["expiry"] = int(float(cookie["expirationDate"]))
                except Exception:
                    pass
            try:
                driver.add_cookie(c)
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th√™m ƒë∆∞·ª£c cookie {cookie.get('name')}: {e}")
                
def expand_all_see_more(driver, post):
    try:
        see_more_btns = post.find_elements(
            By.XPATH,
            ".//div[@role='button' and (contains(text(),'See more') or contains(text(),'Xem th√™m'))]"
        )
        for btn in see_more_btns:
            driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
            try:
                btn.click()
            except Exception:
                driver.execute_script("arguments[0].click();", btn)
            time.sleep(0.6)
    except Exception:
        pass

        
def pick_post_link(post):
    # 1. ∆Øu ti√™n: link ch·ª©a timestamp (th∆∞·ªùng l√† permalink g·ªëc)
    try:
        ts_links = post.find_elements(
            By.XPATH,
            './/a[contains(@href,"permalink") or contains(@href,"story.php")]/span/time/..'
        )
        if ts_links:
            link = ts_links[0].get_attribute("href")
            print(">>> Picked link:", link)   # üëà in ra ƒë·ªÉ debug
            return link
    except Exception:
        pass

    # 2. N·∫øu kh√¥ng c√≥ timestamp th√¨ fallback theo pattern c≈©
    patterns = [
        'contains(@href,"/posts/")',
        'contains(@href,"story.php")',
        'contains(@href,"permalink")',
        'contains(@href,"photo.php")',
        'contains(@href,"/video")'
    ]
    for p in patterns:
        els = post.find_elements(By.XPATH, f'.//a[{p}]')
        if els:
            link = els[0].get_attribute("href")
            print(f">>> Picked link by pattern {p}:", link)
            return link

    # 3. Cu·ªëi c√πng, fallback l·∫•y b·∫•t k·ª≥ link n√†o (√≠t d√πng)
    any_a = post.find_elements(By.XPATH, './/a[@href]')
    return any_a[0].get_attribute('href') if any_a else None

def clean_post_url(url):
    if not url:
        return url
    p = urlparse(url)
    qs = parse_qs(p.query)
    for k in list(qs.keys()):
        if k.startswith("__cft__") or k in {"__tn__","comment_id","mibextid","refid"}:
            qs.pop(k, None)
    q = urlencode(qs, doseq=True)
    return f"{p.scheme}://{p.netloc}{p.path}" + (f"?{q}" if q else "")

NOISE_WORDS = {
    "like","reply","share","comment","send","follow",
    "th√≠ch","tr·∫£ l·ªùi","chia s·∫ª","b√¨nh lu·∫≠n","g·ª≠i","theo d√µi","ph·∫£n h·ªìi"
}
TIME_RE = re.compile(r"^\d+\s*(s|m|h|d|w|y)$", re.I)

def _is_noise(t: str) -> bool:
    s = t.strip()
    if not s:
        return True
    if TIME_RE.match(s):
        return True          # 1d, 3h, 15m...
    if s.isdigit():
        return True               # ‚Äú45‚Äù, ‚Äú2‚Äù (ƒë·∫øm)
    low = s.lower()
    if low in NOISE_WORDS:
        return True        # Like/Reply/Share...
    # d√≤ng r·∫•t ng·∫Øn v√† tr√πng t·ª´ h√†nh ƒë·ªông ‚Üí coi nh∆∞ r√°c
    if len(s) <= 2:
        return True
    return False

def extract_post_text_segments(driver, post):
    expand_all_see_more(driver, post)

    segs = []
    selectors = [
        "div.xdj266r.x14z9mp.xat24cr.x1lziwak.x1vvkbs",    # d√≤ng ƒë·∫ßu
        "div.x14z9mp.xat24cr.x1lziwak.x1vvkbs.xtlvy1s"     # c√°c d√≤ng sau
    ]

    print(">>> ƒêang l·∫•y text cho post...")
    for sel in selectors:
        els = post.find_elements(By.CSS_SELECTOR, sel)
        print(f"Selector {sel} t√¨m th·∫•y {len(els)} elements")
        for el in els:
            print("----", (el.text or '').strip()[:80])

    for sel in selectors:
        for el in post.find_elements(By.CSS_SELECTOR, sel):
            try:
                t = (el.get_attribute("textContent") or "").strip()
                if t:
                    segs.append(t)
            except Exception:
                continue

    # Kh·ª≠ tr√πng l·∫∑p
    seen, uniq = set(), []
    for s in segs:
        if s not in seen:
            seen.add(s)
            uniq.append(s)
    return uniq

def crawl_fanpage():
    options = Options()
    options.add_argument("--disable-notifications")
    options.add_argument("--start-maximized")
    # Tu·ª≥ ch·ªçn gi·∫£m nghi ng·ªù bot (kh√¥ng b·∫Øt bu·ªôc)
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    driver = webdriver.Chrome(service=Service("chromedriver-win64/chromedriver.exe"), options=options)

    driver.get("https://www.facebook.com")
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    load_cookies(driver, COOKIES_FILE)
    driver.refresh()
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

    # V√†o fanpage
    driver.get(PAGE_URL)
    WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "div[role='feed'], div[role='main']"))
    )

    # Cu·ªôn ƒë·ªÉ t·∫£i b√†i v√† ch·ªù ‚Äú·ªïn ƒë·ªãnh‚Äù
    prev = 0         # s·ªë b√†i ƒë√£ load (0 ban ƒë·∫ßu)
    num_scroll = 1   # s·ªë l·∫ßn cu·ªôn, ch·ªânh theo nhu c·∫ßu
    for _ in range(num_scroll):
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3 + random.random())
        cur = len(driver.find_elements(By.CSS_SELECTOR, "div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z"))
        if cur == prev:
            # N·∫øu sau khi cu·ªôn kh√¥ng tƒÉng s·ªë b√†i post, ch·ªù th√™m ch√∫t cho FB load
            time.sleep(1.0)
        prev = cur

    posts = driver.find_elements(By.CSS_SELECTOR, "div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z")
    print(f"üîé T√¨m th·∫•y {len(posts)} b√†i vi·∫øt")

    posts_data = []
    
    for post in posts[:crawl_post]:
        try:
            permalink = clean_post_url(pick_post_link(post))
            segs = extract_post_text_segments(driver, post)
            if not segs:
                continue
            permalink = pick_post_link(post)
            permalink = clean_post_url(permalink)
            posts_data.append({
                "index": len(posts_data) + 1,
                "page_url": PAGE_URL,
                "post_url": permalink or "N/A",
                "segments": segs,
                "post_text": "\n".join(segs)
            })
            print("‚Üí", (segs[0] if segs else "")[:50], permalink)   # in th·ª≠ 50 k√Ω t·ª± ƒë·∫ßu:
        except Exception as e:
            # Gi·ªØ v√≤ng l·∫∑p ch·∫°y ti·∫øp, kh√¥ng b·ªè c·∫£ b√†i
            print("‚ö†Ô∏è L·ªói x·ª≠ l√Ω m·ªôt b√†i:", e)
            continue

    driver.quit()

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(posts_data, f, ensure_ascii=False, indent=4)
    print(f"‚úÖ ƒê√£ l∆∞u {len(posts_data)} b√†i vi·∫øt v√†o {OUTPUT_FILE}")


if __name__ == "__main__":
    crawl_fanpage()
