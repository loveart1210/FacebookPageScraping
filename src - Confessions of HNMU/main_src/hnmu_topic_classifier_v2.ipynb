{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "165a2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "075eac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:  # pragma: no cover\n",
    "    raise RuntimeError(\n",
    "        \"sentence-transformers is required. Install with: pip install sentence-transformers\"\n",
    "    ) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "cec4b2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Notebook config =====\n",
    "INPUT_FILE = \"output/supertest_confessions_of_hnmu.xlsx\"\n",
    "OUTPUT_FILE = \"output/output_supertest_confessions_of_hnmu.json\"\n",
    "MODEL = \"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\"\n",
    "\n",
    "CACHE_DIR = None   # hoặc None\n",
    "LOCAL_ONLY = True\n",
    "\n",
    "MIN_SIM = 0.38\n",
    "MARGIN = 0.05\n",
    "HIGH_CONF = 0.55\n",
    "OOS_GUARD_DELTA = 0.02\n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c3e3806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXONOMY_8: Dict[str, Dict[str, List[str]]] = {\n",
    "    \"Đăng ký học phần\": {\n",
    "        \"keywords\": [\n",
    "            \"đăng ký học phần\", \"đăng ký môn\", \"đăng ký tín chỉ\", \"đăng kí tín chỉ\",\n",
    "            \"chọn lớp\", \"chọn nhóm\", \"hủy đăng ký\", \"huỷ đăng ký\", \"rút học phần\", \"rút môn\",\n",
    "            \"đăng ký lại\", \"học phần tiên quyết\", \"tiên quyết\", \"song hành\", \"học phần điều kiện\",\n",
    "            \"mở lớp\", \"hủy lớp\", \"đóng lớp\", \"đổi lớp\"\n",
    "        ],\n",
    "        \"anchors\": [\n",
    "            \"cách đăng ký học phần, đăng ký tín chỉ, chọn lớp và điều kiện tiên quyết\",\n",
    "            \"rút học phần, hủy đăng ký môn, đăng ký lại học phần khi bị trùng lịch\",\n",
    "            \"không đăng ký được học phần vì tiên quyết/song hành, xử lý như thế nào\"\n",
    "        ],\n",
    "    },\n",
    "    \"Đánh giá - Chấm điểm\": {\n",
    "        \"keywords\": [\n",
    "            \"đánh giá\", \"chấm điểm\", \"thang điểm\", \"điểm quá trình\", \"điểm chuyên cần\",\n",
    "            \"điểm giữa kỳ\", \"điểm cuối kỳ\", \"điểm tổng kết\", \"điểm thành phần\", \"gpa\", \"điểm chữ\",\n",
    "            \"phúc khảo\", \"chấm lại\", \"nhập điểm\", \"sai điểm\", \"đề nghị sửa điểm\"\n",
    "        ],\n",
    "        \"anchors\": [\n",
    "            \"quy định đánh giá và chấm điểm: điểm chuyên cần, điểm quá trình, điểm thi, điểm tổng kết\",\n",
    "            \"phúc khảo bài thi, chấm lại, khi phát hiện sai điểm cần làm thủ tục gì\",\n",
    "            \"cách tính GPA và điểm chữ, quy đổi điểm và điều kiện đạt môn\"\n",
    "        ],\n",
    "    },\n",
    "    \"Học lại - Học cải thiện\": {\n",
    "        \"keywords\": [\n",
    "            \"học lại\", \"học cải thiện\", \"cải thiện điểm\", \"học lại môn\", \"đăng ký học lại\",\n",
    "            \"đăng ký cải thiện\", \"rớt môn\", \"trượt môn\", \"học lại để qua môn\", \"xóa điểm\", \"xoá điểm\"\n",
    "        ],\n",
    "        \"anchors\": [\n",
    "            \"quy định học lại và học cải thiện: khi nào được đăng ký và cách tính điểm\",\n",
    "            \"trượt môn/rớt môn thì đăng ký học lại ra sao, điểm cũ và điểm mới được tính thế nào\",\n",
    "            \"cải thiện điểm để tăng GPA, quy tắc thay thế điểm\"\n",
    "        ],\n",
    "    },\n",
    "    \"Cảnh báo học vụ - Thôi học\": {\n",
    "        \"keywords\": [\n",
    "            \"cảnh báo học vụ\", \"cảnh báo\", \"thôi học\", \"buộc thôi học\",\n",
    "            \"đình chỉ học\", \"đình chỉ\", \"học tiếp\", \"mất quyền học tiếp\",\n",
    "            \"nợ tín chỉ\", \"tín chỉ tích lũy\", \"không đủ điều kiện\", \"gpa thấp\", \"trượt nhiều\"\n",
    "        ],\n",
    "        \"anchors\": [\n",
    "            \"cảnh báo học vụ theo GPA/tín chỉ tích lũy, điều kiện để tiếp tục học\",\n",
    "            \"các mức xử lý học vụ: cảnh báo, đình chỉ, buộc thôi học và cách khắc phục\",\n",
    "            \"mất quyền học tiếp do kết quả học tập, quy trình xét và khiếu nại\"\n",
    "        ],\n",
    "    },\n",
    "    \"Học phí - Miễn giảm\": {\n",
    "        \"keywords\": [\n",
    "            \"học phí\", \"đóng học phí\", \"nợ học phí\", \"miễn giảm\", \"miễn học phí\", \"giảm học phí\",\n",
    "            \"hoàn học phí\", \"hoàn tiền\", \"biên lai\", \"hóa đơn\", \"thu học phí\", \"mức học phí\",\n",
    "            \"phí tín chỉ\", \"phí học phần\"\n",
    "        ],\n",
    "        \"anchors\": [\n",
    "            \"quy định học phí: mức thu, cách tính theo tín chỉ/học phần và thời hạn đóng\",\n",
    "            \"miễn giảm học phí theo đối tượng và hồ sơ cần nộp\",\n",
    "            \"nợ học phí ảnh hưởng đăng ký học phần/kết quả học tập như thế nào\"\n",
    "        ],\n",
    "    },\n",
    "    \"Học bổng - Khen thưởng\": {\n",
    "        \"keywords\": [\n",
    "            \"học bổng\", \"xét học bổng\", \"học bổng khuyến khích\", \"tiêu chí học bổng\",\n",
    "            \"khen thưởng\", \"giải thưởng\", \"hen thưởng\", \"trợ cấp\", \"hỗ trợ\", \"tài trợ\",\n",
    "            \"điểm rèn luyện\", \"điểm rl\"\n",
    "        ],\n",
    "        \"anchors\": [\n",
    "            \"điều kiện và tiêu chí xét học bổng khuyến khích học tập, hồ sơ và thời gian xét\",\n",
    "            \"khen thưởng sinh viên có thành tích, tiêu chuẩn và quy trình xét duyệt\",\n",
    "            \"học bổng tài trợ/trợ cấp, điều kiện nhận và cách nộp hồ sơ\"\n",
    "        ],\n",
    "    },\n",
    "    \"Thực tập - Khóa luận\": {\n",
    "        \"keywords\": [\n",
    "            \"thực tập\", \"intern\", \"thực tập tốt nghiệp\", \"báo cáo thực tập\", \"nhật ký thực tập\",\n",
    "            \"khóa luận\", \"khoá luận\", \"đồ án\", \"đồ án tốt nghiệp\", \"đề cương\", \"đề tài\",\n",
    "            \"giảng viên hướng dẫn\", \"hội đồng\", \"bảo vệ\", \"bảo vệ khóa luận\"\n",
    "        ],\n",
    "        \"anchors\": [\n",
    "            \"quy định thực tập và khóa luận/đồ án: điều kiện, thời gian, hồ sơ và cách đánh giá\",\n",
    "            \"chọn đề tài, đăng ký giảng viên hướng dẫn, nộp đề cương và báo cáo thực tập\",\n",
    "            \"bảo vệ khóa luận/đồ án, hội đồng chấm và tiêu chí đánh giá\"\n",
    "        ],\n",
    "    },\n",
    "    \"Xét và công nhận tốt nghiệp\": {\n",
    "        \"keywords\": [\n",
    "            \"xét tốt nghiệp\", \"xét & công nhận tốt nghiệp\", \"công nhận tốt nghiệp\", \"tốt nghiệp\",\n",
    "            \"điều kiện tốt nghiệp\", \"chuẩn đầu ra\", \"chứng chỉ\", \"ngoại ngữ\", \"tin học\",\n",
    "            \"nợ môn\", \"hoàn thành\", \"bằng tốt nghiệp\", \"cấp bằng\", \"ra trường\"\n",
    "        ],\n",
    "        \"anchors\": [\n",
    "            \"điều kiện xét và công nhận tốt nghiệp: tín chỉ, GPA, chuẩn đầu ra và các chứng chỉ\",\n",
    "            \"quy trình xét tốt nghiệp, thời hạn nộp hồ sơ và xử lý trường hợp còn nợ môn\",\n",
    "            \"cấp bằng tốt nghiệp, bảng điểm và các thủ tục sau khi được công nhận\"\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "TOPIC_ORDER: List[str] = list(TAXONOMY_8.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2ec07a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strong OOS cues for social/personal, buy-sell, rental, love, lost & found, chit-chat\n",
    "OOS_KEYWORDS = [\n",
    "    # greetings / chit-chat\n",
    "    \"chào\", \"hi\", \"hello\", \"mn ơi\", \"mọi người ơi\", \"cho mình hỏi ngoài lề\", \"tâm sự\", \"confession\",\n",
    "    # rental / roommate\n",
    "    \"tìm trọ\", \"roommate\", \"ở ghép\", \"tìm phòng\", \"chung cư\", \"nhà trọ\", \"pass phòng\",\n",
    "    # buy/sell\n",
    "    \"mua bán\", \"thanh lý\", \"pass đồ\", \"bán\", \"mua\", \"order\", \"ship\", \"giveaway\",\n",
    "    # love/relationship\n",
    "    \"crush\", \"ny\", \"người yêu\", \"tình cảm\", \"chia tay\", \"tỏ tình\",\n",
    "    # lost & found (your example)\n",
    "    \"nhặt được\", \"nhặt đc\", \"nhặt dc\", \"đánh rơi\", \"rơi\", \"mất ví\", \"mất thẻ\", \"thẻ sinh viên\",\n",
    "    \"liên hệ mình\", \"ai đánh rơi\", \"ai làm rơi\", \"trả lại\", \"gửi lại\",\n",
    "]\n",
    "\n",
    "OOS_ANCHORS = [\n",
    "    \"câu hỏi xã giao\", \"chào hỏi\", \"trò chuyện cá nhân\", \"tâm sự không liên quan học vụ\",\n",
    "    \"tìm trọ\", \"tìm roommate\", \"mua bán thanh lý đồ\", \"pass đồ\", \"chuyện tình cảm\",\n",
    "    \"nhặt được thẻ sinh viên hoặc đồ vật\", \"nhờ liên hệ để trả lại\", \"đánh rơi thẻ sinh viên hoặc đồ vật\",\n",
    "    \"nhặt được thẻ sinh viên\", \"đánh rơi thẻ sinh viên\", \"mất thẻ sinh viên\", \"nhặt được đồ vật\", \"đánh rơi đồ vật\", \"mất đồ vật\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "ca05679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s: str) -> str:\n",
    "    s = str(s) if s is not None else \"\"\n",
    "    s = s.lower().strip()\n",
    "    # normalize common teen abbreviations to reduce false academic matches\n",
    "    s = s.replace(\" e \", \" em \").replace(\" mk \", \" mình \").replace(\" mn \", \" mọi người \")\n",
    "    return s\n",
    "\n",
    "\n",
    "def contains_oos_keyword(s: str) -> bool:\n",
    "    s = normalize_text(s)\n",
    "    return any(k in s for k in OOS_KEYWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e4c71166",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Thresholds:\n",
    "    min_sim: float = 0.4          # base minimum similarity required\n",
    "    margin: float = 0.04           # best - second_best must exceed this, unless high_conf triggered\n",
    "    high_conf: float = 0.55        # if best >= high_confident, accept even if margin small\n",
    "    oos_guard_delta: float = 0.02  # if oos_similarity >= best_similarity - delta => Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "8e151a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_topic_embeddings(\n",
    "    model: SentenceTransformer,\n",
    "    taxonomy: Dict[str, Dict[str, List[str]]],\n",
    "    topic_ids: List[str],\n",
    "    batch_size: int = 64,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    texts: List[str] = []\n",
    "    keys: List[str] = []\n",
    "    for tid in topic_ids:\n",
    "        anchors = taxonomy[tid].get(\"anchors\", [])\n",
    "        if not anchors:\n",
    "            raise ValueError(f\"Topic {tid} must have non-empty anchors.\")\n",
    "        for a in anchors:\n",
    "            texts.append(a)\n",
    "            keys.append(tid)\n",
    "\n",
    "    embs = model.encode(texts, batch_size=batch_size, normalize_embeddings=True, show_progress_bar=True)\n",
    "    topic_vecs: Dict[str, List[np.ndarray]] = {tid: [] for tid in topic_ids}\n",
    "    for tid, v in zip(keys, embs):\n",
    "        topic_vecs[tid].append(v)\n",
    "\n",
    "    out: Dict[str, np.ndarray] = {}\n",
    "    for tid in topic_ids:\n",
    "        out[tid] = np.mean(np.stack(topic_vecs[tid], axis=0), axis=0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b4965a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_oos_embedding(model: SentenceTransformer, anchors: List[str]) -> np.ndarray:\n",
    "    embs = model.encode(anchors, batch_size=32, normalize_embeddings=True, show_progress_bar=False)\n",
    "    return np.mean(np.stack(embs, axis=0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "62352505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_topics(\n",
    "    model: SentenceTransformer,\n",
    "    text: str,\n",
    "    topic_embeddings: Dict[str, np.ndarray],\n",
    "    topic_ids: List[str],\n",
    ") -> List[Tuple[str, float]]:\n",
    "    x = model.encode([text], normalize_embeddings=True, show_progress_bar=False)\n",
    "    topic_matrix = np.stack([topic_embeddings[tid] for tid in topic_ids], axis=0)\n",
    "    sims = cosine_similarity(x, topic_matrix)[0]  # keep sklearn cosine\n",
    "    scored = [(tid, float(s)) for tid, s in zip(topic_ids, sims)]\n",
    "    scored.sort(key=lambda t: t[1], reverse=True)\n",
    "    return scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "54aade86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(\n",
    "    model: SentenceTransformer,\n",
    "    text: str,\n",
    "    topic_embeddings: Dict[str, np.ndarray],\n",
    "    oos_embedding: np.ndarray,\n",
    "    thresholds: Thresholds,\n",
    ") -> Tuple[str, List[Tuple[str, float]], float]:\n",
    "    norm = normalize_text(text)\n",
    "\n",
    "    # Hard OOS veto first (prevents many false academic hits)\n",
    "    if contains_oos_keyword(norm):\n",
    "        scored = score_topics(model, norm, topic_embeddings, TOPIC_ORDER)\n",
    "        oos_sim = float(cosine_similarity(model.encode([norm], normalize_embeddings=True, show_progress_bar=False),\n",
    "                                          np.stack([oos_embedding], axis=0))[0][0])\n",
    "        return \"T0_OUT_OF_SCOPE\", scored, oos_sim\n",
    "\n",
    "    scored = score_topics(model, norm, topic_embeddings, TOPIC_ORDER)\n",
    "    best_tid, best_sim = scored[0]\n",
    "    second_sim = scored[1][1] if len(scored) > 1 else -1.0\n",
    "    margin = best_sim - second_sim\n",
    "\n",
    "    # OOS similarity guard (semantic out-of-scope)\n",
    "    x = model.encode([norm], normalize_embeddings=True, show_progress_bar=False)\n",
    "    oos_sim = float(cosine_similarity(x, np.stack([oos_embedding], axis=0))[0][0])\n",
    "\n",
    "    # Accept if confident enough AND not semantically close to OOS\n",
    "    confident = (best_sim >= thresholds.high_conf) or (best_sim >= thresholds.min_sim and margin >= thresholds.margin)\n",
    "    if (not confident) or (oos_sim >= best_sim - thresholds.oos_guard_delta):\n",
    "        return \"T0_OUT_OF_SCOPE\", scored, oos_sim\n",
    "\n",
    "    return best_tid, scored, oos_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "178c5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_id: str, cache_dir: Optional[str], local_files_only: bool) -> SentenceTransformer:\n",
    "    kwargs = {}\n",
    "    if cache_dir:\n",
    "        kwargs[\"cache_folder\"] = cache_dir\n",
    "    if local_files_only:\n",
    "        kwargs[\"local_files_only\"] = True\n",
    "\n",
    "    return SentenceTransformer(model_id, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "4a1ff65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name VoVanPhuc/sup-SimCSE-VietNamese-phobert-base. Creating a new one with mean pooling.\n",
      "c:\\Users\\nguye\\.conda\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: output/output_supertest_confessions_of_hnmu.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    args = {\n",
    "        \"input\": INPUT_FILE,\n",
    "        \"output\": OUTPUT_FILE,\n",
    "        \"model\": MODEL,\n",
    "        \"cache_dir\": CACHE_DIR,     # ← THÊM DÒNG NÀY\n",
    "        \"local_only\": LOCAL_ONLY,   # ← THÊM DÒNG NÀY\n",
    "        \"min_sim\": MIN_SIM,\n",
    "        \"margin\": MARGIN,\n",
    "        \"high_conf\": HIGH_CONF,\n",
    "        \"oos_guard_delta\": OOS_GUARD_DELTA,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "    }\n",
    "\n",
    "    df = pd.read_excel(args[\"input\"])\n",
    "\n",
    "    # Rename for standardized interface\n",
    "    if \"index\" in df.columns and \"id\" not in df.columns:\n",
    "        df = df.rename(columns={\"index\": \"id\"})\n",
    "    if \"segment_text\" in df.columns and \"text\" not in df.columns:\n",
    "        df = df.rename(columns={\"segment_text\": \"text\"})\n",
    "\n",
    "    if \"text\" not in df.columns:\n",
    "        raise ValueError(\"Input must contain a 'text' column (or 'segment_text' to be renamed).\")\n",
    "\n",
    "    thresholds = Thresholds(\n",
    "        min_sim=args[\"min_sim\"],\n",
    "        margin=args[\"margin\"],\n",
    "        high_conf=args[\"high_conf\"],\n",
    "        oos_guard_delta=args[\"oos_guard_delta\"],\n",
    "    )\n",
    "\n",
    "    model = load_model(args[\"model\"], args[\"cache_dir\"], args[\"local_only\"])\n",
    "\n",
    "    topic_embeddings = build_topic_embeddings(model, TAXONOMY_8, TOPIC_ORDER, batch_size=args[\"batch_size\"])\n",
    "    oos_embedding = build_oos_embedding(model, OOS_ANCHORS)\n",
    "\n",
    "    labels: List[str] = []\n",
    "    best_scores: List[float] = []\n",
    "    oos_scores: List[float] = []\n",
    "    all_scores_list: List[List] = []\n",
    "\n",
    "    for t in df[\"text\"].fillna(\"\").astype(str).tolist():\n",
    "        label, scored, oos_sim = classify(model, t, topic_embeddings, oos_embedding, thresholds)\n",
    "\n",
    "        labels.append(label)\n",
    "        best_scores.append(scored[0][1] if scored else -1.0)\n",
    "        oos_scores.append(oos_sim)\n",
    "\n",
    "        # Keep all topic scores (8 topics) sorted desc\n",
    "        all_scores_list.append(\n",
    "            [{\"topic\": tid, \"score\": round(score, 6)} for tid, score in scored]\n",
    "        )\n",
    "\n",
    "    df[\"topic_label\"] = labels\n",
    "    df[\"best_topic_score\"] = best_scores\n",
    "    df[\"oos_score\"] = oos_scores\n",
    "    df[\"all_topics_score\"] = all_scores_list\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(args[\"output\"])), exist_ok=True)\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "\n",
    "    with open(args[\"output\"], \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved JSON to: {args['output']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "accaecaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel to: output/output_supertest_confessions_of_hnmu.xlsx\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ===== Config =====\n",
    "INPUT_JSON = \"output/output_supertest_confessions_of_hnmu.json\"\n",
    "OUTPUT_XLSX = \"output/output_supertest_confessions_of_hnmu.xlsx\"\n",
    "\n",
    "# ===== Load JSON =====\n",
    "with open(INPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ===== Convert to DataFrame =====\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ===== Chỉ giữ 2 cột cần thiết =====\n",
    "required_cols = [\"text\", \"topic_label\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "df_out = df[required_cols]\n",
    "\n",
    "# ===== Save to Excel =====\n",
    "os.makedirs(os.path.dirname(os.path.abspath(OUTPUT_XLSX)), exist_ok=True)\n",
    "df_out.to_excel(OUTPUT_XLSX, index=False)\n",
    "\n",
    "print(\"Saved Excel to:\", OUTPUT_XLSX)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
