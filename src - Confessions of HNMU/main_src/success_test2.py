from selenium import webdriver
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import re

from urllib.parse import urlparse, parse_qs, urlencode

import random
import json
import time

# =========================
# ADDED: c·∫•u h√¨nh s·ªë b√†i c·∫ßn crawl (tr∆∞·ªõc ƒë√¢y crawl_post ch∆∞a ƒë·ªãnh nghƒ©a)
# =========================
crawl_post = 2000

PAGE_URL = "https://www.facebook.com/profile.php?id=61555234277669"
OUTPUT_FILE = "output/success_hnmu_posts_2.json"
COOKIES_FILE = "cookies.json"

# Map ƒë√∫ng expiry t·ª´ EditThisCookie (c√≥ tr∆∞·ªùng 'expirationDate')
def load_cookies(driver, cookies_file):
    with open(cookies_file, "r", encoding="utf-8") as f:
        cookies = json.load(f)
        for cookie in cookies:
            c = {
                "name": cookie["name"],
                "value": cookie["value"],
                "domain": cookie.get("domain", ".facebook.com"),
                "path": cookie.get("path", "/"),
                "secure": cookie.get("secure", True),
                "httpOnly": cookie.get("httpOnly", False),
            }
            # EditThisCookie d√πng 'expirationDate' (float gi√¢y). Selenium ch·∫•p nh·∫≠n 'expiry' (int).
            if "expirationDate" in cookie:
                try:
                    c["expiry"] = int(float(cookie["expirationDate"]))
                except Exception:
                    pass
            try:
                driver.add_cookie(c)
            except Exception as e:
                print(f"‚ö†Ô∏è Kh√¥ng th√™m ƒë∆∞·ª£c cookie {cookie.get('name')}: {e}")

# =========================
# FIXED: indentation (h√†m n√†y tr∆∞·ªõc ƒë√≥ b·ªã th·ª•t v√†o trong load_cookies)
# + ADDED: click th√™m ‚ÄúXem b·∫£n d·ªãch / See translation / Xem nguy√™n b·∫£n...‚Äù
# =========================
def expand_all_see_more(driver, post):
    try:
        # M·ªü r·ªông caption
        see_more_btns = post.find_elements(
            By.XPATH,
            ".//div[@role='button' and (contains(.,'See more') or contains(.,'Xem th√™m'))]"
        )
        for btn in see_more_btns:
            try:
                driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
                time.sleep(0.2)
                try:
                    btn.click()
                except Exception:
                    driver.execute_script("arguments[0].click();", btn)
                time.sleep(0.6)
            except Exception:
                continue

        # M·ªü r·ªông b·∫£n d·ªãch / nguy√™n b·∫£n (nhi·ªÅu b√†i n·∫±m sau thao t√°c n√†y)
        translate_btns = post.find_elements(
            By.XPATH,
            ".//div[@role='button' and (contains(.,'Xem b·∫£n d·ªãch') or contains(.,'See translation') or contains(.,'Xem nguy√™n b·∫£n') or contains(.,'See original'))]"
        )
        for btn in translate_btns:
            try:
                driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
                time.sleep(0.2)
                try:
                    btn.click()
                except Exception:
                    driver.execute_script("arguments[0].click();", btn)
                time.sleep(0.6)
            except Exception:
                continue

    except Exception:
        pass

# =========================
# FIXED: indentation (h√†m n√†y tr∆∞·ªõc ƒë√≥ c≈©ng b·ªã th·ª•t sai)
# =========================
def pick_post_link(post):
    # 1. ∆Øu ti√™n: link ch·ª©a timestamp (th∆∞·ªùng l√† permalink g·ªëc)
    try:
        ts_links = post.find_elements(
            By.XPATH,
            './/a[contains(@href,"permalink") or contains(@href,"story.php")]/span/time/..'
        )
        if ts_links:
            link = ts_links[0].get_attribute("href")
            print(">>> Picked link:", link)   # üëà in ra ƒë·ªÉ debug
            return link
    except Exception:
        pass

    # 2. N·∫øu kh√¥ng c√≥ timestamp th√¨ fallback theo pattern c≈©
    patterns = [
        'contains(@href,"/posts/")',
        'contains(@href,"story.php")',
        'contains(@href,"permalink")',
        'contains(@href,"photo.php")',
        'contains(@href,"/video")'
    ]
    for p in patterns:
        els = post.find_elements(By.XPATH, f'.//a[{p}]')
        if els:
            link = els[0].get_attribute("href")
            print(f">>> Picked link by pattern {p}:", link)
            return link

    # 3. Cu·ªëi c√πng, fallback l·∫•y b·∫•t k·ª≥ link n√†o (√≠t d√πng)
    any_a = post.find_elements(By.XPATH, './/a[@href]')
    return any_a[0].get_attribute('href') if any_a else None


def clean_post_url(url):
    if not url:
        return url
    p = urlparse(url)
    qs = parse_qs(p.query)
    for k in list(qs.keys()):
        if k.startswith("__cft__") or k in {"__tn__", "comment_id", "mibextid", "refid"}:
            qs.pop(k, None)
    q = urlencode(qs, doseq=True)
    return f"{p.scheme}://{p.netloc}{p.path}" + (f"?{q}" if q else "")


NOISE_WORDS = {
    "like", "reply", "share", "comment", "send", "follow",
    "th√≠ch", "tr·∫£ l·ªùi", "chia s·∫ª", "b√¨nh lu·∫≠n", "g·ª≠i", "theo d√µi", "ph·∫£n h·ªìi"
}
TIME_RE = re.compile(r"^\d+\s*(s|m|h|d|w|y)$", re.I)


def _is_noise(t: str) -> bool:
    s = t.strip()
    if not s:
        return True
    if TIME_RE.match(s):
        return True          # 1d, 3h, 15m...
    if s.isdigit():
        return True               # ‚Äú45‚Äù, ‚Äú2‚Äù (ƒë·∫øm)
    low = s.lower()
    if low in NOISE_WORDS:
        return True        # Like/Reply/Share...
    # d√≤ng r·∫•t ng·∫Øn v√† tr√πng t·ª´ h√†nh ƒë·ªông ‚Üí coi nh∆∞ r√°c
    if len(s) <= 2:
        return True
    return False


# =========================
# ADDED: H√†m helper l·∫•y caption theo container ·ªïn ƒë·ªãnh
# (KH√îNG x√≥a logic c≈©; ch·ªâ b·ªï sung fallback)
# =========================
def _extract_message_container_text(post):
    msg_nodes = post.find_elements(By.CSS_SELECTOR, 'div[data-ad-preview="message"], div[data-ad-comet-preview="message"]')
    if msg_nodes:
        t = (msg_nodes[0].get_attribute("textContent") or "").strip()
        return t

    msg_nodes2 = post.find_elements(By.CSS_SELECTOR, 'div[data-ad-rendering-role="story_message"]')
    if msg_nodes2:
        t = (msg_nodes2[0].get_attribute("textContent") or "").strip()
        return t

    return ""


def extract_post_text_segments(driver, post):
    expand_all_see_more(driver, post)

    segs = []
    selectors = [
        "div.xdj266r.x14z9mp.xat24cr.x1lziwak.x1vvkbs.x126k92a",    # d√≤ng ƒë·∫ßu
        "div.x14z9mp.xat24cr.x1lziwak.x1vvkbs.xtlvy1s.x126k92a"     # c√°c d√≤ng sau
    ]

    print(">>> ƒêang l·∫•y text cho post...")
    for sel in selectors:
        els = post.find_elements(By.CSS_SELECTOR, sel)
        print(f"Selector {sel} t√¨m th·∫•y {len(els)} elements")
        for el in els:
            print("----", (el.text or '').strip()[:80])

    # ====== Logic c≈© gi·ªØ nguy√™n ======
    for sel in selectors:
        for el in post.find_elements(By.CSS_SELECTOR, sel):
            try:
                t = (el.get_attribute("textContent") or "").strip()
                if t and not _is_noise(t):
                    segs.append(t)
            except Exception:
                continue

    # =========================
    # ADDED: fallback b·ªÅn v·ªØng h∆°n (container message)
    # N·∫øu selector class kh√¥ng b·∫Øt ƒë∆∞·ª£c, v·∫´n l·∫•y ƒë∆∞·ª£c caption
    # =========================
    if not segs:
        t = _extract_message_container_text(post)
        if t:
            # T√°ch theo d√≤ng ƒë·ªÉ t∆∞∆°ng th√≠ch output segments
            lines = [x.strip() for x in t.split("\n") if x.strip()]
            for ln in lines:
                if ln and not _is_noise(ln):
                    segs.append(ln)

    # =========================
    # ADDED: fallback cu·ªëi (dir="auto" trong message container) ƒë·ªÉ h·∫°n ch·∫ø miss do split node
    # =========================
    if not segs:
        msg_nodes = post.find_elements(By.CSS_SELECTOR, 'div[data-ad-preview="message"], div[data-ad-comet-preview="message"], div[data-ad-rendering-role="story_message"]')
        if msg_nodes:
            container = msg_nodes[0]
            for el in container.find_elements(By.CSS_SELECTOR, 'div[dir="auto"]'):
                try:
                    t = (el.get_attribute("textContent") or "").strip()
                    if t and not _is_noise(t):
                        segs.append(t)
                except Exception:
                    continue

    # Kh·ª≠ tr√πng l·∫∑p
    seen, uniq = set(), []
    for s in segs:
        s = " ".join(s.split())  # ADDED: normalize whitespace ƒë·ªÉ gi·∫£m duplicate do kho·∫£ng tr·∫Øng
        if s and s not in seen:
            seen.add(s)
            uniq.append(s)
    return uniq


def crawl_fanpage():
    options = Options()
    options.add_argument("--disable-notifications")
    options.add_argument("--start-maximized")
    # Tu·ª≥ ch·ªçn gi·∫£m nghi ng·ªù bot (kh√¥ng b·∫Øt bu·ªôc)
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    driver = webdriver.Chrome(service=Service(
        "../../chromedriver-win64/chromedriver.exe"), options=options)

    driver.get("https://www.facebook.com")
    WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.TAG_NAME, "body")))
    load_cookies(driver, COOKIES_FILE)
    driver.refresh()
    WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.TAG_NAME, "body")))

    # V√†o fanpage
    driver.get(PAGE_URL)
    WebDriverWait(driver, 20).until(
        EC.presence_of_element_located(
            (By.CSS_SELECTOR, "div[role='feed'], div[role='main']"))
    )

    # Cu·ªôn ƒë·ªÉ t·∫£i b√†i v√† ch·ªù ‚Äú·ªïn ƒë·ªãnh‚Äù
    prev = 0
    cur = 0
    # s·ªë l·∫ßn ch·ªù li√™n ti·∫øp m√† kh√¥ng load th√™m b√†i (ƒë·ªÉ tho√°t n·∫øu h·∫øt b√†i)
    max_wait = 5
    stagnant = 0  # ƒë·∫øm s·ªë l·∫ßn kh√¥ng load th√™m b√†i m·ªõi

    print(f"üìú B·∫Øt ƒë·∫ßu cu·ªôn ƒë·∫øn khi ƒë·ªß {crawl_post} b√†i...")

    while cur < crawl_post:
        driver.execute_script(
            "window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3 + random.random())

        posts = driver.find_elements(
            By.CSS_SELECTOR, "div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z")
        cur = len(posts)
        print(f"üîΩ ƒê√£ load {cur} b√†i...")

        if cur == prev:
            stagnant += 1
            if stagnant >= max_wait:
                print("‚ö†Ô∏è Kh√¥ng th·∫•y b√†i m·ªõi n√†o sau nhi·ªÅu l·∫ßn cu·ªôn, d·ª´ng l·∫°i.")
                break
            time.sleep(2)
        else:
            stagnant = 0  # reset n·∫øu c√≥ b√†i m·ªõi
        prev = cur

    print(f"‚úÖ T·ªïng c·ªông ƒë√£ load {cur} b√†i vi·∫øt.")

    posts = driver.find_elements(
        By.CSS_SELECTOR, "div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z")
    print(f"üîé T√¨m th·∫•y {len(posts)} b√†i vi·∫øt")

    posts_data = []

    for post in posts[:crawl_post]:
        try:
            # 1. Di chuy·ªÉn ƒë·∫øn b√†i vi·∫øt ƒë·ªÉ k√≠ch ho·∫°t Facebook t·∫£i n·ªôi dung
            driver.execute_script(
                "arguments[0].scrollIntoView({block:'center'});", post)
            # 2. Ch·ªù m·ªôt ch√∫t ƒë·ªÉ n·ªôi dung k·ªãp t·∫£i v·ªÅ
            time.sleep(0.7)

            permalink = clean_post_url(pick_post_link(post))
            segs = extract_post_text_segments(driver, post)  # B√¢y gi·ªù m·ªõi tr√≠ch xu·∫•t

            # =========================
            # CHANGED: Kh√¥ng b·ªè qua b√†i khi kh√¥ng c√≥ text
            # M·ª•c ti√™u c·ªßa b·∫°n l√† kh√¥ng b·ªè s√≥t b√†i => v·∫´n l∆∞u record v·ªõi text r·ªóng
            # =========================
            if not segs:
                print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y text (c√≥ th·ªÉ b√†i kh√¥ng c√≥ caption ho·∫∑c DOM kh√°c). V·∫´n l∆∞u v·ªõi post_text=''.")

            posts_data.append({
                "index": len(posts_data) + 1,
                "page_url": PAGE_URL,
                "post_url": permalink or "N/A",
                "segments": segs,
                "post_text": "\n".join(segs) if segs else ""
            })
            print("‚Üí", (segs[0] if segs else "")[:50], permalink)

        except Exception as e:
            print("‚ö†Ô∏è L·ªói x·ª≠ l√Ω m·ªôt b√†i:", e)
            # ADDED: v·∫´n l∆∞u ‚Äúkhung‚Äù ƒë·ªÉ tr√°nh m·∫•t b√†i ho√†n to√†n (t√πy b·∫°n; gi·ªØ ƒë√∫ng m·ª•c ti√™u kh√¥ng b·ªè s√≥t)
            try:
                permalink = None
                try:
                    permalink = clean_post_url(pick_post_link(post))
                except Exception:
                    permalink = None
                posts_data.append({
                    "index": len(posts_data) + 1,
                    "page_url": PAGE_URL,
                    "post_url": permalink or "N/A",
                    "segments": [],
                    "post_text": ""
                })
            except Exception:
                pass
            continue

    driver.quit()

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(posts_data, f, ensure_ascii=False, indent=4)
    print(f"‚úÖ ƒê√£ l∆∞u {len(posts_data)} b√†i vi·∫øt v√†o {OUTPUT_FILE}")


if __name__ == "__main__":
    crawl_fanpage()
