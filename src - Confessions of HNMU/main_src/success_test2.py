from selenium import webdriver
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.remote.client_config import ClientConfig
from webdriver_manager.chrome import ChromeDriverManager
import re, os

from urllib.parse import urlparse, parse_qs, urlencode

import random
import json
import time

PAGE_URL = "https://www.facebook.com/profile.php?id=61555234277669"
OUTPUT_JSONL_FILE = "output/2_confessions_of_hnmu.jsonl"   # output chÃ­nh (JSONL)
OUTPUT_JSON_FILE  = "output/2_confessions_of_hnmu.json"    # file JSON array sau khi convert
CHECKPOINT_FILE   = "output/2_checkpoint_hnmu.json"
COOKIES_FILE = "cookies.json"

# Sá»‘ bÃ i muá»‘n crawl
crawl_post = 3000

def load_cookies(driver, cookies_file):
    with open(cookies_file, "r", encoding="utf-8") as f:
        cookies = json.load(f)
        for cookie in cookies:
            c = {
                "name": cookie["name"],
                "value": cookie["value"],
                "domain": cookie.get("domain", ".facebook.com"),
                "path": cookie.get("path", "/"),
                "secure": cookie.get("secure", True),
                "httpOnly": cookie.get("httpOnly", False),
            }
            # EditThisCookie dÃ¹ng 'expirationDate' (float giÃ¢y). Selenium cháº¥p nháº­n 'expiry' (int).
            if "expirationDate" in cookie:
                try:
                    c["expiry"] = int(float(cookie["expirationDate"]))
                except Exception:
                    pass
            try:
                driver.add_cookie(c)
            except Exception as e:
                print(f"âš ï¸ KhÃ´ng thÃªm Ä‘Æ°á»£c cookie {cookie.get('name')}: {e}")


def expand_all_see_more(driver, post):
    try:
        # Má»Ÿ rá»™ng caption
        see_more_btns = post.find_elements(
            By.XPATH,
            ".//div[@role='button' and (contains(.,'See more') or contains(.,'Xem thÃªm'))]"
        )
        for btn in see_more_btns:
            try:
                driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
                time.sleep(0.2)
                try:
                    btn.click()
                except Exception:
                    driver.execute_script("arguments[0].click();", btn)
                time.sleep(0.6)
            except Exception:
                continue

        # Má»Ÿ rá»™ng báº£n dá»‹ch / nguyÃªn báº£n (nhiá»u bÃ i náº±m sau thao tÃ¡c nÃ y)
        translate_btns = post.find_elements(
            By.XPATH,
            ".//div[@role='button' and (contains(.,'Xem báº£n dá»‹ch') or contains(.,'See translation') or contains(.,'Xem nguyÃªn báº£n') or contains(.,'See original'))]"
        )
        for btn in translate_btns:
            try:
                driver.execute_script("arguments[0].scrollIntoView({block:'center'});", btn)
                time.sleep(0.2)
                try:
                    btn.click()
                except Exception:
                    driver.execute_script("arguments[0].click();", btn)
                time.sleep(0.6)
            except Exception:
                continue

    except Exception:
        pass

def pick_post_link(post):
    # 1. Æ¯u tiÃªn: link chá»©a timestamp (thÆ°á»ng lÃ  permalink gá»‘c)
    try:
        ts_links = post.find_elements(
            By.XPATH,
            './/a[contains(@href,"permalink") or contains(@href,"story.php")]/span/time/..'
        )
        if ts_links:
            link = ts_links[0].get_attribute("href")
            print(">>> Picked link:", link)   # ğŸ‘ˆ in ra Ä‘á»ƒ debug
            return link
    except Exception:
        pass

    # 2. Náº¿u khÃ´ng cÃ³ timestamp thÃ¬ fallback theo pattern cÅ©
    patterns = [
        'contains(@href,"/posts/")',
        'contains(@href,"story.php")',
        'contains(@href,"permalink")',
        'contains(@href,"photo.php")',
        'contains(@href,"/video")'
    ]
    for p in patterns:
        els = post.find_elements(By.XPATH, f'.//a[{p}]')
        if els:
            link = els[0].get_attribute("href")
            print(f">>> Picked link by pattern {p}:", link)
            return link

    # 3. Cuá»‘i cÃ¹ng, fallback láº¥y báº¥t ká»³ link nÃ o (Ã­t dÃ¹ng)
    any_a = post.find_elements(By.XPATH, './/a[@href]')
    return any_a[0].get_attribute('href') if any_a else None

def clean_post_url(url):
    if not url:
        return url
    p = urlparse(url)
    qs = parse_qs(p.query)
    for k in list(qs.keys()):
        if k.startswith("__cft__") or k in {"__tn__", "comment_id", "mibextid", "refid"}:
            qs.pop(k, None)
    q = urlencode(qs, doseq=True)
    return f"{p.scheme}://{p.netloc}{p.path}" + (f"?{q}" if q else "")

def save_checkpoint(processed, seen_urls):
    os.makedirs(os.path.dirname(CHECKPOINT_FILE), exist_ok=True)
    with open(CHECKPOINT_FILE, "w", encoding="utf-8") as f:
        json.dump({"processed": int(processed), "seen_urls": list(seen_urls)}, f, ensure_ascii=False)

def load_checkpoint():
    try:
        with open(CHECKPOINT_FILE, "r", encoding="utf-8") as f:
            d = json.load(f)
            return int(d.get("processed", 0)), set(d.get("seen_urls", []))
    except Exception:
        return 0, set()
    
NOISE_WORDS = {
    "like", "reply", "share", "comment", "send", "follow",
    "thÃ­ch", "tráº£ lá»i", "chia sáº»", "bÃ¬nh luáº­n", "gá»­i", "theo dÃµi", "pháº£n há»“i"
}
TIME_RE = re.compile(r"^\d+\s*(s|m|h|d|w|y)$", re.I)

def _is_noise(t: str) -> bool:
    s = t.strip()
    if not s:
        return True
    if TIME_RE.match(s):
        return True          # 1d, 3h, 15m...
    if s.isdigit():
        return True               # â€œ45â€, â€œ2â€ (Ä‘áº¿m)
    low = s.lower()
    if low in NOISE_WORDS:
        return True        # Like/Reply/Share...
    # dÃ²ng ráº¥t ngáº¯n vÃ  trÃ¹ng tá»« hÃ nh Ä‘á»™ng â†’ coi nhÆ° rÃ¡c
    if len(s) <= 2:
        return True
    return False

def _extract_message_container_text(post):
    msg_nodes = post.find_elements(By.CSS_SELECTOR, 'div[data-ad-preview="message"], div[data-ad-comet-preview="message"]')
    if msg_nodes:
        t = (msg_nodes[0].get_attribute("textContent") or "").strip()
        return t

    msg_nodes2 = post.find_elements(By.CSS_SELECTOR, 'div[data-ad-rendering-role="story_message"]')
    if msg_nodes2:
        t = (msg_nodes2[0].get_attribute("textContent") or "").strip()
        return t

    return ""

def extract_post_text_segments(driver, post):
    expand_all_see_more(driver, post)

    segs = []
    selectors = [
        "div.xdj266r.x14z9mp.xat24cr.x1lziwak.x1vvkbs.x126k92a",    # dÃ²ng Ä‘áº§u
        "div.x14z9mp.xat24cr.x1lziwak.x1vvkbs.xtlvy1s.x126k92a"     # cÃ¡c dÃ²ng sau
    ]

    print(">>> Äang láº¥y text cho post...")
    for sel in selectors:
        els = post.find_elements(By.CSS_SELECTOR, sel)
        print(f"Selector {sel} tÃ¬m tháº¥y {len(els)} elements")
        for el in els:
            print("----", (el.text or '').strip()[:80])

    # ====== Logic cÅ© giá»¯ nguyÃªn ======
    for sel in selectors:
        for el in post.find_elements(By.CSS_SELECTOR, sel):
            try:
                t = (el.get_attribute("textContent") or "").strip()
                if t and not _is_noise(t):
                    segs.append(t)
            except Exception:
                continue

    # =========================
    # ADDED: fallback bá»n vá»¯ng hÆ¡n (container message)
    # Náº¿u selector class khÃ´ng báº¯t Ä‘Æ°á»£c, váº«n láº¥y Ä‘Æ°á»£c caption
    # =========================
    if not segs:
        t = _extract_message_container_text(post)
        if t:
            # TÃ¡ch theo dÃ²ng Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch output segments
            lines = [x.strip() for x in t.split("\n") if x.strip()]
            for ln in lines:
                if ln and not _is_noise(ln):
                    segs.append(ln)

    # =========================
    # ADDED: fallback cuá»‘i (dir="auto" trong message container) Ä‘á»ƒ háº¡n cháº¿ miss do split node
    # =========================
    if not segs:
        msg_nodes = post.find_elements(By.CSS_SELECTOR, 'div[data-ad-preview="message"], div[data-ad-comet-preview="message"], div[data-ad-rendering-role="story_message"]')
        if msg_nodes:
            container = msg_nodes[0]
            for el in container.find_elements(By.CSS_SELECTOR, 'div[dir="auto"]'):
                try:
                    t = (el.get_attribute("textContent") or "").strip()
                    if t and not _is_noise(t):
                        segs.append(t)
                except Exception:
                    continue

    # Khá»­ trÃ¹ng láº·p
    seen, uniq = set(), []
    for s in segs:
        s = " ".join(s.split())  # ADDED: normalize whitespace Ä‘á»ƒ giáº£m duplicate do khoáº£ng tráº¯ng
        if s and s not in seen:
            seen.add(s)
            uniq.append(s)
    return uniq

def build_driver():
    options = Options()
    options.add_argument("--disable-notifications")
    options.add_argument("--start-maximized")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)

    # Giáº£m táº£i: táº¯t áº£nh, háº¡n cháº¿ autoplay
    prefs = {
        "profile.managed_default_content_settings.images": 2,
        "profile.default_content_setting_values.notifications": 2,
        "autoplay-policy": "document-user-activation-required",
    }
    options.add_experimental_option("prefs", prefs)
    options.add_argument("--disable-gpu")

    # TÄƒng timeout command Selenium â†” ChromeDriver (Selenium 4.38)
    client_config = ClientConfig(timeout=300)

    driver = webdriver.Chrome(
        service=Service("../../chromedriver-win64/chromedriver.exe"),
        options=options,
        client_config=client_config
    )

    driver.get("https://www.facebook.com")
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
    load_cookies(driver, COOKIES_FILE)
    driver.refresh()
    WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, "body")))

    driver.get(PAGE_URL)
    WebDriverWait(driver, 20).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "div[role='feed'], div[role='main']"))
    )
    return driver

def crawl_fanpage():
    processed, seen_urls = load_checkpoint()
    driver = build_driver()

    # Cuá»™n Ä‘á»ƒ táº£i bÃ i vÃ  chá» â€œá»•n Ä‘á»‹nhâ€
    max_wait = 5
    stagnant = 0

    os.makedirs(os.path.dirname(OUTPUT_JSONL_FILE), exist_ok=True)

    mode = "a" if (processed > 0 and os.path.exists(OUTPUT_JSONL_FILE)) else "w"
    with open(OUTPUT_JSONL_FILE, mode, encoding="utf-8") as f:
        print(f"ğŸ“œ Báº¯t Ä‘áº§u cuá»™n vÃ  xá»­ lÃ½ Ä‘áº¿n khi Ä‘á»§ {crawl_post} bÃ i... (resume={processed})")

        while processed < crawl_post:
            # --- TIMEOUT SELF-HEAL: náº¿u find_elements bá»‹ Read timed out thÃ¬ restart ---
            try:
                posts = driver.find_elements(By.CSS_SELECTOR, "div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z")
            except Exception as e:
                msg = str(e).lower()
                if "read timed out" in msg or "httppool" in msg:
                    print("âš ï¸ ChromeDriver timeout. Restart vÃ  resume...")
                    save_checkpoint(processed, seen_urls)
                    try:
                        driver.quit()
                    except Exception:
                        pass
                    driver = None
                    driver = build_driver()
                    continue
                raise

            cur = len(posts)
            print(f"ğŸ”½ Äang tháº¥y {cur} post trÃªn DOM | Ä‘Ã£ lÆ°u {processed}")

            if cur <= processed:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(3 + random.random())

                posts2 = driver.find_elements(By.CSS_SELECTOR, "div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z")
                cur2 = len(posts2)
                if cur2 <= cur:
                    stagnant += 1
                    if stagnant >= max_wait:
                        print("âš ï¸ KhÃ´ng tháº¥y post má»›i, dá»«ng.")
                        break
                else:
                    stagnant = 0
                continue

            for i in range(processed, min(cur, crawl_post)):
                try:
                    # refetch theo index Ä‘á»ƒ giáº£m stale
                    posts = driver.find_elements(By.CSS_SELECTOR, "div.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z")
                    if i >= len(posts):
                        break
                    post = posts[i]

                    driver.execute_script("arguments[0].scrollIntoView({block:'center'});", post)
                    time.sleep(0.7)

                    permalink = clean_post_url(pick_post_link(post)) or "N/A"

                    # chá»‘ng trÃ¹ng do FB re-render
                    if permalink != "N/A" and permalink in seen_urls:
                        processed += 1
                        continue
                    if permalink != "N/A":
                        seen_urls.add(permalink)

                    segs = extract_post_text_segments(driver, post)
                    if not segs:
                        print("âš ï¸ KhÃ´ng tÃ¬m tháº¥y text... váº«n lÆ°u post_text=''")

                    data = {
                        "index": processed + 1,
                        "page_url": PAGE_URL,
                        "post_url": permalink,
                        "segments": segs,
                        "post_text": "\n".join(segs) if segs else ""
                    }

                    # --- JSONL: má»—i dÃ²ng 1 JSON object ---
                    f.write(json.dumps(data, ensure_ascii=False) + "\n")
                    f.flush()

                    # --- dá»n DOM Ä‘á»ƒ giáº£m RAM/Ä‘Æ¡ ---
                    try:
                        driver.execute_script("arguments[0].remove();", post)
                    except Exception:
                        pass

                    processed += 1

                    # checkpoint má»—i 10 bÃ i
                    if processed % 10 == 0:
                        save_checkpoint(processed, seen_urls)
                        try:
                            f.flush()
                            os.fsync(f.fileno())
                        except Exception:
                            pass

                except Exception as e:
                    print("âš ï¸ Lá»—i xá»­ lÃ½ má»™t bÃ i:", e)
                    data = {
                        "index": processed + 1,
                        "page_url": PAGE_URL,
                        "post_url": "N/A",
                        "segments": [],
                        "post_text": ""
                    }
                    f.write(json.dumps(data, ensure_ascii=False) + "\n")
                    f.flush()

                    processed += 1
                    if processed % 10 == 0:
                        save_checkpoint(processed, seen_urls)
                        try:
                            f.flush()
                            os.fsync(f.fileno())
                        except Exception:
                            pass
                    continue

        save_checkpoint(processed, seen_urls)
        print(f"âœ… ÄÃ£ lÆ°u {processed} bÃ i viáº¿t vÃ o {OUTPUT_JSONL_FILE}")


    driver.quit()

def jsonl_to_json(jsonl_path=OUTPUT_JSONL_FILE, json_path=OUTPUT_JSON_FILE):
    items = []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            items.append(json.loads(line))

    os.makedirs(os.path.dirname(json_path), exist_ok=True)
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(items, f, ensure_ascii=False, indent=4)

    print(f"âœ… Convert xong: {len(items)} records -> {json_path}")

if __name__ == "__main__":
    crawl_fanpage()
    jsonl_to_json()